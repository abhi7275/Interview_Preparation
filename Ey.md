Can you provide an overview of your experience working with PySpark and big data processing?

2. What motivated you to specialize in PySpark, and how have you applied it in your previous roles?

3. Explain the basic architecture of PySpark.

4. How does PySpark relate to Apache Spark, and what advantages does it offer in distributed data processing?

5. Describe the difference between a DataFrame and an RDD in PySpark.

6. Can you explain transformations and actions in PySpark DataFrames?

7. Provide examples of PySpark DataFrame operations you frequently use.

8. How do you optimize the performance of PySpark jobs?

9. Can you discuss techniques for handling skewed data in PySpark?

10. Explain how data serialization works in PySpark.

11. Discuss the significance of choosing the right compression codec for your PySpark applications.

12. How do you deal with missing or null values in PySpark DataFrames?

13. Are there any specific strategies or functions you prefer for handling missing data?
14. Scenario-Based: "You need to run a pipeline only when a file arrives in the source." I explained how to set up event-based triggers in Data Factory.

5.Explain Spark Architecture: A question focused on the internals of Spark, including its components and execution flow.

6.Broadcast Join: Discussing when and why to use broadcast joins in Spark to optimize performance.

7.SCD Types: An exploration of Slowly Changing Dimensions (SCD) and how they are managed in data warehousing.

8.Coding Question: "Suppose a previous employee is rejoining the company. His employee ID and work email ID change, whereas address and personal email ID remain the same. They maintain historical data, and I need to get the latest data for the employee. How do you do that?" This question tested my SQL and data manipulation skills, focusing on how to handle updates and maintain historical integrity.
1. Can you provide an overview of your experience working with PySpark and big data processing?
2. What motivated you to specialize in PySpark, and how have you applied it in your previous roles?

ğğ²ğ’ğ©ğšğ«ğ¤ ğğšğ¬ğ¢ğœğ¬:
3. Explain the basic architecture of PySpark.
4. How does PySpark relate to Apache Spark, and what advantages does it offer in distributed data processing?

ğƒğšğ­ğšğ…ğ«ğšğ¦ğ ğğ©ğğ«ğšğ­ğ¢ğ¨ğ§ğ¬:
5. Describe the difference between a DataFrame and an RDD in PySpark.
6. Can you explain transformations and actions in PySpark DataFrames?
7. Provide examples of PySpark DataFrame operations you frequently use.

ğğ©ğ­ğ¢ğ¦ğ¢ğ³ğ¢ğ§ğ  ğğ²ğ’ğ©ğšğ«ğ¤ ğ‰ğ¨ğ›ğ¬:
8. How do you optimize the performance of PySpark jobs?
9. Can you discuss techniques for handling skewed data in PySpark?

ğƒğšğ­ğš ğ’ğğ«ğ¢ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ ğšğ§ğ ğ‚ğ¨ğ¦ğ©ğ«ğğ¬ğ¬ğ¢ğ¨ğ§:
10. Explain how data serialization works in PySpark.
11. Discuss the significance of choosing the right compression codec for your PySpark applications.

ğ‡ğšğ§ğğ¥ğ¢ğ§ğ  ğŒğ¢ğ¬ğ¬ğ¢ğ§ğ  ğƒğšğ­ğš:
12. How do you deal with missing or null values in PySpark DataFrames?
13. Are there any specific strategies or functions you prefer for handling missing data?

ğ–ğ¨ğ«ğ¤ğ¢ğ§ğ  ğ°ğ¢ğ­ğ¡ ğğ²ğ’ğ©ğšğ«ğ¤ ğ’ğğ‹:
14. Describe your experience with PySpark SQL.
15. How do you execute SQL queries on PySpark DataFrames?

ğğ«ğ¨ğšğğœğšğ¬ğ­ğ¢ğ§ğ  ğ¢ğ§ ğğ²ğ’ğ©ğšğ«ğ¤:
16. What is broadcasting, and how is it useful in PySpark?
17. Provide an example scenario where broadcasting can significantly improve performance.

ğğ²ğ’ğ©ğšğ«ğ¤ ğŒğšğœğ¡ğ¢ğ§ğ ğ‹ğğšğ«ğ§ğ¢ğ§ğ :
18. Discuss your experience with PySpark's MLlib.
19. Can you give examples of machine learning algorithms you've implemented using PySpark?

ğ‰ğ¨ğ› ğŒğ¨ğ§ğ¢ğ­ğ¨ğ«ğ¢ğ§ğ  ğšğ§ğ ğ‹ğ¨ğ ğ ğ¢ğ§ğ :
20. How do you monitor and troubleshoot PySpark jobs?
21. Describe the importance of logging in PySpark applications.

ğˆğ§ğ­ğğ ğ«ğšğ­ğ¢ğ¨ğ§ ğ°ğ¢ğ­ğ¡ ğğ­ğ¡ğğ« ğ“ğğœğ¡ğ§ğ¨ğ¥ğ¨ğ ğ¢ğğ¬:
22. Have you integrated PySpark with other big data technologies or databases? If so, please provide examples.
23. How do you handle data transfer between PySpark and external systems?

ğ‘ğğšğ¥-ğ°ğ¨ğ«ğ¥ğ ğğ«ğ¨ğ£ğğœğ­ ğ’ğœğğ§ğšğ«ğ¢ğ¨:
24. Explain the project that you worked on in your previous organizations.
25. Describe a challenging PySpark project you've worked on. What were the key challenges, and how did you overcome them?

ğ‚ğ¥ğ®ğ¬ğ­ğğ« ğŒğšğ§ğšğ ğğ¦ğğ§ğ­:
26. Explain your experience with cluster management in PySpark.
27. How do you scale PySpark applications in a cluster environment?

Broadcasting in spark 2. How does spark handle fault tolerance 3. Why spark is preferred over MapReduce 4. Two Scenarios where you had to do spark optimizations 5. Significance of partitions in spark 6. What is window function? 7. group by vs window function which would you prefer 8. Types of joins 9. Other optimization techniques in spark 10. AQE in spark 11. pyspark code for getting the latest transaction for every user 12. read a csv file in pyspark 13. Various pyspark Dataframe operations code 14. What is git add?


- Cluster Configuration and Tasks:
- ï»¿ï»¿- Each node is of size - 16 CPU cores / 64 GB RAM.
- ï»¿ï»¿- Each node has 3 executors, with each executor of size - 5 CPU cores / 21  
    GB RAM.
- ï»¿ï»¿1. What's the total capacity of the cluster?
- ï»¿ï»¿2. How many parallel tasks can run on this cluster?
- ï»¿ï»¿3. Let's say you requested 4 executors; how many parallel tasks can run?
- ï»¿ï»¿4. If you read a 10.1 GB CSV file stored in a data lake and need to filter the data, how many tasks will run?
- ï»¿ï»¿5. Is there a possibility of an out of memory error in the above scenario?

Given a list of integers, write a Scala function to filter out even numbers and then square each of the

remaining numbers. List = (1, 2, 3, 4, 5, 6)


- ï»¿ï»¿Given the following DataFrame:
- ï»¿ï»¿Name Salary Location Designation
- ï»¿ï»¿Sushil 20000 Kolkata Developer
- ï»¿ï»¿Vishal 35000 Bangalore Lead
- ï»¿ï»¿Sritama 50000 Gurgaon Manager
- ï»¿ï»¿Nandini 24000 Hyderabad Developer
- ï»¿ï»¿Vinod 52000 Bangalore Manager
- ï»¿ï»¿Asha 70000 Mumbai Senior Manager

Add the prefix â€œemp_â€,  to each columns of above Dataframe 
. at output, Name should become Emp_Name, Salary should necome Emp_Salary and so on



â€¢ Implement a function that flattens a nested list structure. For example, given a list 'List (List(1, 2, 3), List(4, 5), List(List(6, 7))) *, the function should return

*List(1, 2, 3, 4, 5, 6, 7):


Given a DataFrame with columns id', name', age,

"salary, and 'department', perform the following transformations:

- ï»¿ï»¿- Filter out records where age is less than 30.
- ï»¿ï»¿- Group the remaining records by department and calculate the average salary for each department.
- ï»¿ï»¿- Sort the result by the average salary in descending order.

