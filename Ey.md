Can you provide an overview of your experience working with PySpark and big data processing?

2. What motivated you to specialize in PySpark, and how have you applied it in your previous roles?

3. Explain the basic architecture of PySpark.

4. How does PySpark relate to Apache Spark, and what advantages does it offer in distributed data processing?

5. Describe the difference between a DataFrame and an RDD in PySpark.

6. Can you explain transformations and actions in PySpark DataFrames?

7. Provide examples of PySpark DataFrame operations you frequently use.

8. How do you optimize the performance of PySpark jobs?

9. Can you discuss techniques for handling skewed data in PySpark?

10. Explain how data serialization works in PySpark.

11. Discuss the significance of choosing the right compression codec for your PySpark applications.

12. How do you deal with missing or null values in PySpark DataFrames?

13. Are there any specific strategies or functions you prefer for handling missing data?
14. Scenario-Based: "You need to run a pipeline only when a file arrives in the source." I explained how to set up event-based triggers in Data Factory.

5.Explain Spark Architecture: A question focused on the internals of Spark, including its components and execution flow.

6.Broadcast Join: Discussing when and why to use broadcast joins in Spark to optimize performance.

7.SCD Types: An exploration of Slowly Changing Dimensions (SCD) and how they are managed in data warehousing.

8.Coding Question: "Suppose a previous employee is rejoining the company. His employee ID and work email ID change, whereas address and personal email ID remain the same. They maintain historical data, and I need to get the latest data for the employee. How do you do that?" This question tested my SQL and data manipulation skills, focusing on how to handle updates and maintain historical integrity.
1. Can you provide an overview of your experience working with PySpark and big data processing?
2. What motivated you to specialize in PySpark, and how have you applied it in your previous roles?

𝐏𝐲𝐒𝐩𝐚𝐫𝐤 𝐁𝐚𝐬𝐢𝐜𝐬:
3. Explain the basic architecture of PySpark.
4. How does PySpark relate to Apache Spark, and what advantages does it offer in distributed data processing?

𝐃𝐚𝐭𝐚𝐅𝐫𝐚𝐦𝐞 𝐎𝐩𝐞𝐫𝐚𝐭𝐢𝐨𝐧𝐬:
5. Describe the difference between a DataFrame and an RDD in PySpark.
6. Can you explain transformations and actions in PySpark DataFrames?
7. Provide examples of PySpark DataFrame operations you frequently use.

𝐎𝐩𝐭𝐢𝐦𝐢𝐳𝐢𝐧𝐠 𝐏𝐲𝐒𝐩𝐚𝐫𝐤 𝐉𝐨𝐛𝐬:
8. How do you optimize the performance of PySpark jobs?
9. Can you discuss techniques for handling skewed data in PySpark?

𝐃𝐚𝐭𝐚 𝐒𝐞𝐫𝐢𝐚𝐥𝐢𝐳𝐚𝐭𝐢𝐨𝐧 𝐚𝐧𝐝 𝐂𝐨𝐦𝐩𝐫𝐞𝐬𝐬𝐢𝐨𝐧:
10. Explain how data serialization works in PySpark.
11. Discuss the significance of choosing the right compression codec for your PySpark applications.

𝐇𝐚𝐧𝐝𝐥𝐢𝐧𝐠 𝐌𝐢𝐬𝐬𝐢𝐧𝐠 𝐃𝐚𝐭𝐚:
12. How do you deal with missing or null values in PySpark DataFrames?
13. Are there any specific strategies or functions you prefer for handling missing data?

𝐖𝐨𝐫𝐤𝐢𝐧𝐠 𝐰𝐢𝐭𝐡 𝐏𝐲𝐒𝐩𝐚𝐫𝐤 𝐒𝐐𝐋:
14. Describe your experience with PySpark SQL.
15. How do you execute SQL queries on PySpark DataFrames?

𝐁𝐫𝐨𝐚𝐝𝐜𝐚𝐬𝐭𝐢𝐧𝐠 𝐢𝐧 𝐏𝐲𝐒𝐩𝐚𝐫𝐤:
16. What is broadcasting, and how is it useful in PySpark?
17. Provide an example scenario where broadcasting can significantly improve performance.

𝐏𝐲𝐒𝐩𝐚𝐫𝐤 𝐌𝐚𝐜𝐡𝐢𝐧𝐞 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠:
18. Discuss your experience with PySpark's MLlib.
19. Can you give examples of machine learning algorithms you've implemented using PySpark?

𝐉𝐨𝐛 𝐌𝐨𝐧𝐢𝐭𝐨𝐫𝐢𝐧𝐠 𝐚𝐧𝐝 𝐋𝐨𝐠𝐠𝐢𝐧𝐠:
20. How do you monitor and troubleshoot PySpark jobs?
21. Describe the importance of logging in PySpark applications.

𝐈𝐧𝐭𝐞𝐠𝐫𝐚𝐭𝐢𝐨𝐧 𝐰𝐢𝐭𝐡 𝐎𝐭𝐡𝐞𝐫 𝐓𝐞𝐜𝐡𝐧𝐨𝐥𝐨𝐠𝐢𝐞𝐬:
22. Have you integrated PySpark with other big data technologies or databases? If so, please provide examples.
23. How do you handle data transfer between PySpark and external systems?

𝐑𝐞𝐚𝐥-𝐰𝐨𝐫𝐥𝐝 𝐏𝐫𝐨𝐣𝐞𝐜𝐭 𝐒𝐜𝐞𝐧𝐚𝐫𝐢𝐨:
24. Explain the project that you worked on in your previous organizations.
25. Describe a challenging PySpark project you've worked on. What were the key challenges, and how did you overcome them?

𝐂𝐥𝐮𝐬𝐭𝐞𝐫 𝐌𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭:
26. Explain your experience with cluster management in PySpark.
27. How do you scale PySpark applications in a cluster environment?

Broadcasting in spark 2. How does spark handle fault tolerance 3. Why spark is preferred over MapReduce 4. Two Scenarios where you had to do spark optimizations 5. Significance of partitions in spark 6. What is window function? 7. group by vs window function which would you prefer 8. Types of joins 9. Other optimization techniques in spark 10. AQE in spark 11. pyspark code for getting the latest transaction for every user 12. read a csv file in pyspark 13. Various pyspark Dataframe operations code 14. What is git add?


- Cluster Configuration and Tasks:
- ﻿﻿- Each node is of size - 16 CPU cores / 64 GB RAM.
- ﻿﻿- Each node has 3 executors, with each executor of size - 5 CPU cores / 21  
    GB RAM.
- ﻿﻿1. What's the total capacity of the cluster?
- ﻿﻿2. How many parallel tasks can run on this cluster?
- ﻿﻿3. Let's say you requested 4 executors; how many parallel tasks can run?
- ﻿﻿4. If you read a 10.1 GB CSV file stored in a data lake and need to filter the data, how many tasks will run?
- ﻿﻿5. Is there a possibility of an out of memory error in the above scenario?

Given a list of integers, write a Scala function to filter out even numbers and then square each of the

remaining numbers. List = (1, 2, 3, 4, 5, 6)


- ﻿﻿Given the following DataFrame:
- ﻿﻿Name Salary Location Designation
- ﻿﻿Sushil 20000 Kolkata Developer
- ﻿﻿Vishal 35000 Bangalore Lead
- ﻿﻿Sritama 50000 Gurgaon Manager
- ﻿﻿Nandini 24000 Hyderabad Developer
- ﻿﻿Vinod 52000 Bangalore Manager
- ﻿﻿Asha 70000 Mumbai Senior Manager

Add the prefix “emp_”,  to each columns of above Dataframe 
. at output, Name should become Emp_Name, Salary should necome Emp_Salary and so on



• Implement a function that flattens a nested list structure. For example, given a list 'List (List(1, 2, 3), List(4, 5), List(List(6, 7))) *, the function should return

*List(1, 2, 3, 4, 5, 6, 7):


Given a DataFrame with columns id', name', age,

"salary, and 'department', perform the following transformations:

- ﻿﻿- Filter out records where age is less than 30.
- ﻿﻿- Group the remaining records by department and calculate the average salary for each department.
- ﻿﻿- Sort the result by the average salary in descending order.

